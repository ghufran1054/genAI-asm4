{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the Chairman of NETSOL Technologies is not explicitly mentioned. However, the Annual General Meeting (AGM) is going to be held on October 18, 2024, and the Chairman is typically the person who chairs such meetings. If you need to know the current Chairman of NETSOL Technologies, I would recommend checking the company's website, their latest annual report, or contacting the company directly at corporate@netsolpk.com for the most accurate and up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_groq import ChatGroq\n",
    "from tavily import TavilyClient\n",
    "# from langchain.vectorstores import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from pinecone import Pinecone\n",
    "from typing import Annotated, Literal, TypedDict\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "PINE_API_KEY = os.getenv('PINE_API_KEY')\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Question: {question}\\n\\nContext: {context}\\n\\nAnswer:\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=PINE_API_KEY,\n",
    ")\n",
    "index = pc.Index(\n",
    "    'netsol-finance-asm4'\n",
    ")\n",
    "vector_store = PineconeVectorStore(index=index, embedding=HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\"))\n",
    "model = ChatGroq(api_key=GROQ_API_KEY)\n",
    "tavily = TavilyClient(api_key=TAVILY_API_KEY)\n",
    "\n",
    "\n",
    "\n",
    "def category_decider(state: MessagesState) -> Literal[\"vector_store\", \"tavily\"]:\n",
    "    \"\"\"\n",
    "    A simple rule based checker to see in which category the user query lies\n",
    "    \"\"\"\n",
    "    message = state['messages'][-1].content\n",
    "    message = message.lower()\n",
    "\n",
    "    if \"netsol\" in message:\n",
    "        return \"vector_store\"\n",
    "    else:\n",
    "        return \"tavily\" \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def pinecone_retriever(state: MessagesState):\n",
    "\n",
    "    query = state['messages'][-1].content\n",
    "    retriever = vector_store.as_retriever()\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    return {\n",
    "        \"messages\": [query, \" \".join([doc.page_content for doc in docs])] \n",
    "        }\n",
    "\n",
    "def tavily_retriever(state: MessagesState):\n",
    "    query = state['messages'][-1].content\n",
    "    results = tavily.search(query)['results']\n",
    "    return {\n",
    "        \"messages\":[query, \" \".join([f\"Content fetched from web with Title: {result['title']}\\n and content: {result['content']}\" for result in results])]\n",
    "    }\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    message = state['messages'][-1].content\n",
    "    query = state['messages'][0].content\n",
    "\n",
    "    messages = [\n",
    "        (\n",
    "            'system',\n",
    "            \"You are an LLM agent you will answer user queries based on the context provided.\"\n",
    "        ),\n",
    "        (\n",
    "            'human',\n",
    "            f'My Query is {query} Please answer Based on this context: {message}'\n",
    "        )\n",
    "    ]\n",
    "    response = model.invoke(\n",
    "        messages\n",
    "    )\n",
    "    return {\"messages\": [response.content]}\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tavily\", tavily_retriever)\n",
    "workflow.add_node(\"vector_store\", pinecone_retriever)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    START, \n",
    "    category_decider\n",
    ")\n",
    "workflow.add_edge(\"tavily\", \"agent\")\n",
    "workflow.add_edge(\"vector_store\", \"agent\")\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Running the pipeline\n",
    "query = \"What awards has NETSOL Technologies won?\"\n",
    "# query = \"Who is the CEO of Tesla ?\"\n",
    "final_response = app.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=query)]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": 40}}\n",
    ")\n",
    "print(final_response['messages'][-1].content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
